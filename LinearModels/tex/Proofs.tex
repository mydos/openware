% Copyright Â© 2012 Edward O'Callaghan. All Rights Reserved.

\documentclass[10pt, oneside, reqno]{amsart}

\usepackage{../../texstyle}

% TODO: break into new file: Proofs
	
\title{Proofs}                               % Document Title
\author{Edward O'Callaghan}
\date{}                                      % Activate to display a given date or no date

\begin{document}
\maketitle \tableofcontents \clearpage

\input{maximumlikelihoodestimators.tex}

\input{distributiontheory.tex}

\section{Coefficient of determination (Proof)} % (fold)
\label{sec:proofone}
Given that:
\begin{defn}[$\emph{total}$ sum of squares]
 \begin{displaymath}
  SS_{total} = \sum_{i=1}^{n} (y_i - \bar{y})^2
 \end{displaymath}
\end{defn}

\begin{defn}[$\emph{reg}$ression sum of squares]
 \begin{displaymath}
  SS_{reg} = \sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2
 \end{displaymath}
\end{defn}

\begin{defn}[$\emph{res}$idual sum of squares]
 \begin{displaymath}
  SS_{res} = \sum_{i=1}^{n} (y_{i} - \hat{y_{i}})^2
 \end{displaymath}
\end{defn}


\begin{prob}[Show that:]
	$ SS_{total} = SS_{reg} + SS_{res} $
\end{prob}

A direct proof by construction is given:
\begin{proof}
\begin{align*}
 \sum_{i} (\hat{y_i} - \bar{y})^2 + \sum_{i} (y_i - \hat{y_i})^2
 & =
 \sum_{i} ( \underbrace{ \hat{y_i} - \bar{y} }_{a} )^2 + ( \underbrace{ y_i - \hat{y_i} }_{b} )^2
 \tag{and recall that $a^2 + b^2 = (a+b)^2 - 2ab$}
 \\
 & = \sum_{i} (y_i - \bar{y})^2 - 2 \underbrace{ \sum_{i} (\hat{y_i} - \bar{y})(y_i - \hat{y_i})}_{\text{show this } =\, 0}
 \intertext{and so, take:}
 \sum_{i} (\hat{y_i} - \bar{y})(y_i - \hat{y_i}) & \underset{\uparrow}{=}
 \sum_{i} ( \cancel{\bar{y}} + b_1 (x_i - \bar{x}) - \cancel{\bar{y}})[y_i - \bar{y} - b_1 (x_i - \bar{x})]
 \\
 & \text{by, } \hat{y_i} = b_0 + b_1 x_i = (\bar{y} -b_1 \bar{x}) + b_1 x_i = \bar{y} + b_1(x_i - \bar{x}) % FIXME: make this a bit more clear.. (multi-line)
 \\
 & = b_1 \sum_{i} (x_i - \bar{x}) \left[ y_i - \bar{y} - b_1 (x_i - \bar{x}) \right]
 \\
 & = b_1 \left[ S_{xy} - b_1 S_{xx} \right]
 \\
 & = b_1 \left[ S_{xy} - \frac{S_{xy}}{\cancel{S_{xx}}} \cancel{S_{xx}} \right] = 0. \qedhere
\end{align*}
\end{proof}

\begin{exmp}
	Suppose $\mathbf{Y}$ is a multivariate normal random vector with expectation
	$\mathbb{E}[\mathbf{Y}]=0$ and covariance matrix $V$. Given that $\mathbf{Y}$
	has length $n$. Prove that $\mathbf{Y}^{\top} V^{-1} \mathbf{Y}$ has
	chi-squared distribution with $n$-degrees of freedom. Prove also that $V$ is
	always semi-positive definite.
	\begin{proof}
		\begin{align*}
			\mathbf{Y}^{\top} V^{-1} \mathbf{Y} &= \mathbf{Y}^{\top}
			\left( \mathbb{E} \left[
				( \mathbf{Y} - \boldsymbol{\mu}_{Y} )
				( \mathbf{Y} - \boldsymbol{\mu}_{Y} )^{\top}
			\right] \right)^{-1} \mathbf{Y}
			\intertext{where $\boldsymbol{\mu}_{Y} = \mathbb{E}[\mathbf{Y}]$.
			Now, suppose we have the Cholesky decomposition $V^{-1} = Q^{\top} Q$ so that,}
			\mathbf{Y}^{\top} V^{-1} \mathbf{Y} &= \mathbf{Y}^{\top} Q^{\top} Q \mathbf{Y}
			\\
			&= (Q \mathbf{Y})^{\top} Q \mathbf{Y}
			\intertext{now let $Q \mathbf{Y} = Z$ and so we have,}
			\Rightarrow \mathbf{Y}^{\top} V^{-1} \mathbf{Y} &= Z^{2}.
			\intertext{Observe that,}
			\mathbb{E}[Q \mathbf{Y}] &= Q \mathbb{E}[\mathbf{Y}] = \mathbf{0}
			\tag{since $Q$ is a constant matrix}
			\intertext{and that,}
			Var(Q \mathbf{Y}) &= Q Var(\mathbf{Y}) Q^{\top}
			\\
			&= Q (Q^{\top} Q)^{-1} Q^{\top} = I_n.
			\intertext{Since $Q$ is both square and invertible and that $Z$ is multivariate
			standard normal,}
			Z & \sim \mathcal{N}(\mathbf{0},I_n)
			\intertext{so then $\mathbf{Y}^{\top} V^{-1} \mathbf{Y}$ has chi-squared
		distribution with $n$-degrees of freedom.}
		\end{align*}
	\end{proof}
\end{exmp}

\end{document}% End of document.
